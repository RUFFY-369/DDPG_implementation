{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf  \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from replay_buffer import replay_buffer\n",
    "from nets import actor,critic\n",
    "from ou_noise import ou_action_noise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementation of Deep Deterministic Policy Gradient(DDPG)\n",
    "# : https://arxiv.org/abs/1509.02971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    def __init__(self,alpha=0.001,beta = 0.002,inp_dims = [8],env=None,gamma= 0.99,n_actions=2,\n",
    "                 max_size=1000000,tau = 0.005,fcl1=512,fcl2=512,batch_size = 64):\n",
    "                 \n",
    "        self.gamma =gamma\n",
    "        self.tau = tau\n",
    "        self.mem = replay_buffer(max_size,inp_dims,n_actions)\n",
    "        self.batch_size= batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.min_action = env.action_space.low[0]\n",
    "        self.actor =actor(n_actions=n_actions,name = \"Actor\")\n",
    "        self.critic = critic(n_actions=n_actions,name=\"Critic\")\n",
    "                 \n",
    "        self.actor_target = actor(n_actions=n_actions,name=\"Target_actor\")\n",
    "        self.critic_target = critic(n_actions=n_actions,name=\"Target_critic\")\n",
    "                 \n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))     #learning rate is given for formality to compile\n",
    "        self.critic.compile(optimizer=Adam(learning_rate=beta))\n",
    "        self.actor_target.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic_target.compile(optimizer=Adam(learning_rate=beta))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.noise = ou_action_noise(mu=np.zeros(n_actions))\n",
    "\n",
    "\n",
    "        self.update_net_params(tau=1)\n",
    "    \n",
    "    def update_net_params(self,tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights=[]\n",
    "\n",
    "        targets = self.actor_target.weights\n",
    "        for i,w in enumerate(self.actor.weights):\n",
    "            weights.append(w*tau +targets[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights)\n",
    "\n",
    "        weights=[]\n",
    "        targets=self.critic_target.weights\n",
    "        for i,w in enumerate(self.critic.weights):\n",
    "            weights.append(w*tau +targets[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights)\n",
    "\n",
    "    def rem_transition(self,state,action,reward,nw_state,done):\n",
    "        self.mem.transition_store(state,action,reward,nw_state,done)\n",
    "\n",
    "    def model_save(self):\n",
    "        print(\"..........Saving the model..........\")\n",
    "        self.actor.save_weights(self.actor.checkpoint_file)\n",
    "        self.actor_target.save_weights(self.actor_target.checkpoint_file)\n",
    "        self.critic.save_weights(self.critic.checkpoint_file)\n",
    "        self.critic_target.save_weights(self.critic_target.checkpoint_file)\n",
    "\n",
    "    def load_model(self):\n",
    "        print(\"...........Loading the model..........\")\n",
    "        self.actor.load_weights(self.actor.checkpoint_file)\n",
    "        self.actor_target.load_weights(self.actor_target.checkpoint_file)\n",
    "        self.critic.load_weights(self.critic.checkpoint_file)\n",
    "        self.critic_target.load_weights(self.critic_target.checkpoint_file)\n",
    "\n",
    "    def action_choose(self,obsv,evaluate=False):\n",
    "        states = tf.convert_to_tensor([obsv],dtype=tf.float32)\n",
    "        actions = self.actor(states)\n",
    "        #if not evaluate:        #they used Ornstein Uhlenbeck(can add class for this noise)\n",
    "            #actions+= tf.random.normal(shape=[self.n_actions],mean=0.0,stddev=0.1)\n",
    "        #actions = tf.clip_by_value(actions,self.min_action,self.max_action)\n",
    "\n",
    "        noise =self.noise()                #\n",
    "        actions_prime = actions + noise    #  #used Ornstein Uhlenbeck\n",
    "        return actions_prime[0]            #\n",
    "        #return actions[0]\n",
    "\n",
    "    def learning(self):\n",
    "        if self.mem.count_mem < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state,action,reward,nw_state,done = self.mem.sample_buffer(self.batch_size)\n",
    "        states= tf.convert_to_tensor(state,dtype=tf.float32)\n",
    "        nw_states = tf.convert_to_tensor(nw_state,dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward,dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action,dtype= tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_target = self.actor_target(nw_states)\n",
    "            nw_critic_val = tf.squeeze(self.critic_target(nw_states,actions_target),1)\n",
    "            critic_val=tf.squeeze(self.critic(states,actions),1)\n",
    "            target = reward + self.gamma*nw_critic_val*(1-done)\n",
    "            critic_loss = keras.losses.MSE(target,critic_val)\n",
    "\n",
    "        critic_net_grad = tape.gradient(critic_loss,self.critic.trainable_variables)\n",
    "\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_net_grad,self.critic.trainable_variables))\n",
    "\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_pol_acs = self.actor(states)\n",
    "            actor_loss = -self.critic(states,new_pol_acs)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "        \n",
    "        actor_net_grad = tape.gradient(actor_loss,self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_net_grad,self.actor.trainable_variables))\n",
    "\n",
    "        self.update_net_params()\n",
    "\n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
